#! /bin/bash

set -x

BRICKS_PER_NODE=3
REPLICAS=3
BRICK_HOSTDIR=/etc/kubernetes/bricks

echo "Generating new StorageClass"

# Retrieve GlusterFS pod IPs
PEER_IPS=$(kubectl get pods -o wide | grep glusterfs | grep -v provisioner | awk '{print $6}' | sort)
BRICK_PATHS=""

for i in $(seq 0 $(($BRICKS_PER_NODE - 1))); do
    for pod_ip in ${PEER_IPS[@]}; do
      # Insert comma if we already started accumlating ips/paths
      if [ "$BRICK_PATHS" != "" ]; then
        BRICK_PATHS="$BRICK_PATHS,"
      fi

      # Build up brickrootPaths one host at a time
      NEW_BRICK="${BRICK_HOSTDIR}/brick${i}"
      BRICK_PATHS="${BRICK_PATHS}${pod_ip}:${NEW_BRICK}"
    done
done

cat <<EOF | tee ./glusterfs/storageclass.yaml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: glusterfs-simple
provisioner: gluster.org/glusterfs-simple
reclaimPolicy: Retain
parameters:
  forceCreate: "true"
  volumeType: "replica $REPLICAS"
  brickrootPaths: "$BRICK_PATHS"
EOF

kubectl delete -f ./glusterfs/storageclass.yaml
kubectl apply -f ./glusterfs/storageclass.yaml

PVC_MIGRATION_LIST=$(kubectl get pvc --no-headers | awk '{print $1}' )

for PVC in $PVC_MIGRATION_LIST; do
    echo $PVC
    PV=$(kubectl get pvc --no-headers $PVC | awk '{print $3}')
    VOLUME_SIZE=$(kubectl get pvc --no-headers $PVC | awk '{print $4}')
    ACCESS_MODE_SHORT=$(kubectl get pvc --no-headers $PVC | awk '{print $5}')

    ACCESS_MODE="ReadWriteOnce"
    if [[ $ACCESS_MODE_SHORT == "RWX" ]]; then
        ACCESS_MODE="ReadWriteMany"
    fi

    kubectl delete pvc $PVC
    kubectl delete pv $PV

    cat <<EOF | kubectl apply -f -
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ${PVC}
  annotations:
    volume.beta.kubernetes.io/storage-class: "glusterfs-simple"
spec:
  accessModes:
    - ${ACCESS_MODE}
  resources:
    requests:
      storage: ${VOLUME_SIZE}
EOF

    ## Create PV & PVC
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ${PV}-tmp
  annotations:
    volume.beta.kubernetes.io/storage-class: "glusterfs-simple"
    pv.kubernetes.io/provisioned-by: gluster.org/glusterfs-simple
spec:
  capacity:
    storage: ${VOLUME_SIZE}
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  claimRef:
    namespace: default
    name: ${PVC}-tmp
  glusterfs:
    endpoints: glusterfs-simple-${PVC}
    path: ${PV}
EOF

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ${PVC}-tmp
  annotations:
    volume.beta.kubernetes.io/storage-class: "glusterfs-simple"
spec:
  volumeName: ${PV}-tmp
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: ${VOLUME_SIZE}
EOF

cat <<EOF | kubectl apply -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: copier-${PVC}
spec:
  template:
    spec:
      containers:
      - name: copier-${PVC}
        image: alpine
        command: ["cp", "-r", "/mnt/pvc-tmp/.", "/mnt/pvc"]
        volumeMounts:
          - name: pvc-tmp
            mountPath: /mnt/pvc-tmp
          - name: pvc
            mountPath: /mnt/pvc
      restartPolicy: OnFailure
      volumes:
        - name: pvc-tmp
          persistentVolumeClaim:
            claimName: ${PVC}-tmp
        - name: pvc
          persistentVolumeClaim:
            claimName: ${PVC}
  backoffLimit: 4
EOF

COMPLETED=$(kubectl get jobs copier-${PVC} --no-headers | awk '{print $2}')
while [[ $COMPLETED -ne "1/1" ]]; do
    sleep 4
    COMPLETED=$(kubectl get jobs copier-${PVC} --no-headers | awk '{print $2}')
    echo $COMPLETED
done

kubectl delete job copier-${PVC}
kubectl delete pvc ${PVC}-tmp

done
