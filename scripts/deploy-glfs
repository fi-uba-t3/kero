#!/bin/bash

set +x

function usage() {
    echo "Usage: $0 <num_replicas> <bricks_per_node>"
    exit 1
}

if [[ -z "$1" ]]; then
    usage
fi

if [[ -z "$2" ]]; then
    usage
fi

REPLICAS=$1
BRICKS_PER_NODE=$2

STORAGE_NODES=$(kubectl get nodes --no-headers | awk '{print $1}')
NODE_COUNT=$(echo $STORAGE_NODES | wc -w)

# DEBUG ONLY: Set this to "echo" to neuter the script and perform a dry-run
DEBUG=""
 
# The host directory to store brick files
BRICK_HOSTDIR="/etc/kubernetes/bricks"

# Ensure that we have enough storage nodes to run GLFS
if [ "$NODE_COUNT" -lt 2 ]; then
  echo "ERROR: Cannot deploy GlusterFS with less than 2 nodes"
  exit 1
fi

sudo untaint-nodes
 
# Label storage nodes appropriately
for node in $STORAGE_NODES; do
  $DEBUG kubectl label nodes $node storagenode=glusterfs --overwrite
done
 
# Create the GLFS cluster
$DEBUG kubectl apply -f $KERO_HOME/services/glusterfs/glusterfs-daemonset.yaml
 
# Wait for the GLFS cluster to come up
count="$(kubectl get pods --no-headers | grep glusterfs | grep -v provisioner | awk '{print $3}' | grep Running | wc -l)"
while [ "$count" -lt "$NODE_COUNT" ]; do
  echo "Waiting for GLFS: $count / $NODE_COUNT"
  sleep 5
  count="$(kubectl get pods --no-headers | grep glusterfs | grep -v provisioner | sed -e s/[\\n\\r]//g | awk '{print $3}' | grep -o Running | wc -l)"
done
echo "GlusterFS is now Running: $count / $NODE_COUNT"
 
# Retrieve GlusterFS pod IPs
PEER_IPS=$(kubectl get pods -o wide | grep glusterfs | grep -v provisioner | awk '{print $6}' | sort)
 
# Use pod names / IPs to exec in and perform `gluster peer probe`
for pod_ip in ${PEER_IPS}; do
  for peer_ip in ${PEER_IPS}; do
    # Skip each node probing itself
    if [ "$pod_ip" == "$peer_ip" ]; then
      continue;
    fi

    # Perform a gluster peer probe
    pod_name=$(kubectl get pods -o wide | grep $pod_ip | awk '{print $1}')
    $DEBUG kubectl exec -it $pod_name gluster peer probe $peer_ip
  done;
done;
 
# Dynamically build StorageClass from pod IPs (see below)
BRICK_PATHS=""

for i in $(seq 0 $(($BRICKS_PER_NODE - 1))); do
    for pod_ip in ${PEER_IPS[@]}; do
      # Insert comma if we already started accumlating ips/paths
      if [ "$BRICK_PATHS" != "" ]; then
        BRICK_PATHS="$BRICK_PATHS,"
      fi

      # Build up brickrootPaths one host at a time
      NEW_BRICK="${BRICK_HOSTDIR}/brick${i}"
      BRICK_PATHS="${BRICK_PATHS}${pod_ip}:${NEW_BRICK}"
    done
done
 
# Modify StorageClass to contain our GlusterFS brickrootPaths
echo "---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: glusterfs-simple
provisioner: gluster.org/glusterfs-simple
reclaimPolicy: Retain
parameters:
  forceCreate: \"true\"
  volumeType: \"replica $REPLICAS\"
  brickrootPaths: \"$BRICK_PATHS\"
" > $KERO_HOME/services/glusterfs/storageclass.yaml
 
# Create the storage class
$DEBUG kubectl delete -f $KERO_HOME/services/glusterfs/storageclass.yaml
$DEBUG kubectl apply -f $KERO_HOME/services/glusterfs/storageclass.yaml
 
# Bind the necessary ServiceAccount / ClusterRole
$DEBUG kubectl apply -f $KERO_HOME/services/glusterfs/rbac.yaml
 
# Create the GLFS Simple Provisioner
$DEBUG kubectl apply -f $KERO_HOME/services/glusterfs/deployment.yaml
